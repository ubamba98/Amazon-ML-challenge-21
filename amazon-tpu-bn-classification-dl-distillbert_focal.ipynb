{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# https://huggingface.co/transformers/custom_datasets.html","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version 1.7 --apt-packages libomp5 libopenblas-dev\n!pip install numpy","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-01T14:03:49.973764Z","iopub.execute_input":"2021-08-01T14:03:49.974246Z","iopub.status.idle":"2021-08-01T14:05:02.398374Z","shell.execute_reply.started":"2021-08-01T14:03:49.974164Z","shell.execute_reply":"2021-08-01T14:05:02.397263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget https://s3-ap-southeast-1.amazonaws.com/he-public-data/dataset52a7b21.zip\n!unzip dataset52a7b21.zip\n!rm dataset/.~lock.train.csv#\n!rm dataset52a7b21.zip","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-01T14:05:11.071494Z","iopub.execute_input":"2021-08-01T14:05:11.071858Z","iopub.status.idle":"2021-08-01T14:07:05.098845Z","shell.execute_reply.started":"2021-08-01T14:05:11.071824Z","shell.execute_reply":"2021-08-01T14:07:05.097542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\nimport numpy as np\nimport pickle\nimport pandas as pd\ntrain = pd.read_csv(\"dataset/train.csv\", escapechar = \"\\\\\", quoting = csv.QUOTE_NONE)\ntest = pd.read_csv(\"dataset/test.csv\", escapechar = \"\\\\\", quoting = csv.QUOTE_NONE)\n\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T14:07:05.101206Z","iopub.execute_input":"2021-08-01T14:07:05.101647Z","iopub.status.idle":"2021-08-01T14:07:45.882846Z","shell.execute_reply.started":"2021-08-01T14:07:05.101596Z","shell.execute_reply":"2021-08-01T14:07:45.881935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_counts = train.BROWSE_NODE_ID.value_counts()\ndrop_indices = class_counts[class_counts<50].index\ntrain = train[~train.BROWSE_NODE_ID.isin(drop_indices)]\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T14:07:45.884703Z","iopub.execute_input":"2021-08-01T14:07:45.885Z","iopub.status.idle":"2021-08-01T14:07:46.354004Z","shell.execute_reply.started":"2021-08-01T14:07:45.884963Z","shell.execute_reply":"2021-08-01T14:07:46.352611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_split, val_split = train_test_split(train, test_size=.05)\nfrom transformers import DistilBertTokenizerFast\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2021-08-01T14:07:46.355301Z","iopub.execute_input":"2021-08-01T14:07:46.355534Z","iopub.status.idle":"2021-08-01T14:07:52.640979Z","shell.execute_reply.started":"2021-08-01T14:07:46.35551Z","shell.execute_reply":"2021-08-01T14:07:52.639953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_map = {}\nfor idx, value in enumerate(train.BROWSE_NODE_ID.unique()):\n    label_map[value] = idx\n\nwith open('lable_map.pickle', 'wb') as handle:\n    pickle.dump(label_map, handle, protocol=pickle.HIGHEST_PROTOCOL)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T14:07:52.642583Z","iopub.execute_input":"2021-08-01T14:07:52.643237Z","iopub.status.idle":"2021-08-01T14:07:52.684818Z","shell.execute_reply.started":"2021-08-01T14:07:52.64319Z","shell.execute_reply":"2021-08-01T14:07:52.684023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# class FocalLoss(nn.Module):\n#     def __init__(self,reduction, gamma=2, eps=1e-7):\n#         super(FocalLoss, self).__init__()\n#         self.gamma = gamma\n#         #print(self.gamma)\n#         self.eps = eps\n#         self.ce = torch.nn.CrossEntropyLoss(reduction=reduction)\n\n#     def forward(self, input, target):\n#         logp = self.ce(input, target.to(torch.long))\n#         p = torch.exp(-logp)\n#         loss = (1 - p) ** self.gamma * logp\n#         return loss.mean()\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass FocalLoss(nn.modules.loss._WeightedLoss):\n    def __init__(self, weight=None, gamma=2,reduction='mean'):\n        super(FocalLoss, self).__init__(weight,reduction=reduction)\n        self.gamma = gamma\n        self.weight = weight #weight parameter will act as the alpha parameter to balance class weights\n\n    def forward(self, input, target):\n\n        ce_loss = F.cross_entropy(input, target,reduction=self.reduction,weight=self.weight)\n        pt = torch.exp(-ce_loss)\n        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n        return focal_loss","metadata":{"execution":{"iopub.status.busy":"2021-08-01T14:07:52.686072Z","iopub.execute_input":"2021-08-01T14:07:52.686537Z","iopub.status.idle":"2021-08-01T14:07:52.692742Z","shell.execute_reply.started":"2021-08-01T14:07:52.686505Z","shell.execute_reply":"2021-08-01T14:07:52.691932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import nn\nfrom transformers import Trainer\n\nclass CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n        loss_fct = FocalLoss()\n        loss = loss_fct(logits.view(-1, self.model.num_labels),\n                        labels.float().view(-1, 1).squeeze())\n        return (loss, outputs) if return_outputs else loss","metadata":{"execution":{"iopub.status.busy":"2021-08-01T14:07:52.693858Z","iopub.execute_input":"2021-08-01T14:07:52.694302Z","iopub.status.idle":"2021-08-01T14:07:57.892099Z","shell.execute_reply.started":"2021-08-01T14:07:52.694269Z","shell.execute_reply":"2021-08-01T14:07:57.891268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, df, tokenizer, is_train=True, label_map={}, max_length=128):\n        self.df = df\n        self.brand = df.BRAND.values\n        self.title = df.TITLE.values\n        self.desc = df.DESCRIPTION.values\n        self.bullets = df.BULLET_POINTS.apply(lambda x: x[1:-1] if len(x)>0 and x[0]=='[' else x).values\n        self.tokenizer = tokenizer\n        if is_train:\n            self.labels = df.BROWSE_NODE_ID.apply(lambda x: label_map[x]).values\n            self.label_map = label_map\n        self.is_train = is_train\n        self.max_length = max_length\n \n    def __getitem__(self, idx):\n        \n#         req_string = self.brand[idx] + '~'\n        req_string = self.title[idx] + ' ~ '\n        if torch.rand(1)>0.5:\n            req_string += self.desc[idx]\n        req_string += ' ~ '\n        if torch.rand(1)>0.5:\n            req_string += self.bullets[idx]\n        tokenized_data = tokenizer.tokenize(req_string)\n        to_append = [\"[CLS]\"] + tokenized_data[:self.max_length - 2] + [\"[SEP]\"]\n        input_ids = tokenizer.convert_tokens_to_ids(to_append)\n        input_mask = [1] * len(input_ids)\n        padding = [0] * (self.max_length - len(input_ids))\n        input_ids += padding\n        input_mask += padding\n        item = {\n            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n            \"attention_mask\": torch.tensor(input_mask, dtype=torch.long)\n        }\n        if self.is_train:\n            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n        return item\n \n    def __len__(self):\n        return len(self.df)\n\ntrain_dataset = Dataset(train_split.fillna(\"\"), tokenizer, is_train=True, label_map=label_map)\nval_dataset = Dataset(val_split.fillna(\"\"), tokenizer, is_train=True, label_map=label_map)\ntest_dataset = Dataset(test.fillna(\"\"), tokenizer, is_train=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T14:07:57.893912Z","iopub.execute_input":"2021-08-01T14:07:57.894433Z","iopub.status.idle":"2021-08-01T14:08:05.319058Z","shell.execute_reply.started":"2021-08-01T14:07:57.894395Z","shell.execute_reply":"2021-08-01T14:08:05.31807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir='./results',          # output directory\n    max_steps=15500,                  # total number of training epochs\n    per_device_train_batch_size=256,\n    # batch size per device during training\n    per_device_eval_batch_size=256,  # batch size for evaluation\n    warmup_steps=1000,               # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,               # strength of weight decay\n    logging_dir='./logs',            # directory for storing logs\n    logging_steps=500,\n    dataloader_num_workers=4,\n    report_to=\"tensorboard\",\n    label_smoothing_factor=0.1,\n    tpu_num_cores=8,\n    evaluation_strategy=\"steps\",\n    eval_steps=5000, # Evaluation and Save happens every 500 steps\n    save_strategy = \"steps\",\n    save_steps = 1000,\n    save_total_limit=3, # Only last 5 models are saved. Older ones are deleted.\n    load_best_model_at_end=True,   #best model is always saved\n    prediction_loss_only = True,\n)\n\nmodel = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\nmodel.classifier = torch.nn.Linear(768, len(label_map))\nmodel.num_labels = len(label_map)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T14:08:05.320368Z","iopub.execute_input":"2021-08-01T14:08:05.320637Z","iopub.status.idle":"2021-08-01T14:08:22.294313Z","shell.execute_reply.started":"2021-08-01T14:08:05.320609Z","shell.execute_reply":"2021-08-01T14:08:22.29336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainer = Trainer(\n#     model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n#     args=training_args,                  # training arguments, defined above\n#     train_dataset=train_dataset,         # training dataset\n#     eval_dataset=val_dataset             # evaluation dataset\n# )\ntrainer = CustomTrainer(model = model , args  = training_args , train_dataset = train_dataset , eval_dataset = val_dataset)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T14:08:22.295725Z","iopub.execute_input":"2021-08-01T14:08:22.296117Z","iopub.status.idle":"2021-08-01T16:21:36.609841Z","shell.execute_reply.started":"2021-08-01T14:08:22.296073Z","shell.execute_reply":"2021-08-01T16:21:36.608796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}